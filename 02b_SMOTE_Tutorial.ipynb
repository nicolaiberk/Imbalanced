{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopie von Kopie von SMOTE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM8ezrj2qfds+i2kHyhK9ZX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaiberk/Imbalanced/blob/master/02b_SMOTE_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic Minority Oversampling TEchnique - SMOTE"
      ],
      "metadata": {
        "id": "t4Wxmq4d7DG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The solution for this notebook can be found [here](https://colab.research.google.com/github/nicolaiberk/Imbalanced/blob/master/02b_SMOTE_Solution.ipynb).**"
      ],
      "metadata": {
        "id": "wfD_1ggBSzZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to address imbalanced data problems is to generate additional artificial cases. One method to do this is called **S**ynthetic **M**inority **O**versampling **Te**chnique - or short: **SMOTE**. The full paper can be found [here](https://arxiv.org/abs/1106.1813), a less technical explainer [here](https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/) and the scikit-learn documentation on oversampling [here](https://imbalanced-learn.org/stable/over_sampling.html)."
      ],
      "metadata": {
        "id": "0yQ-aRJcc1uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a highly imbalanced dataset of tweets that were annotated using crowd-coding from [this paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3026393). The tweets are classified according to whether they discuss the refugee movements in 2015. We will use the `imblearn` package to implement SMOTE.\n",
        "\n",
        "We start by importing the packages and loading the data. "
      ],
      "metadata": {
        "id": "Rq6QkKXterPD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7hq4zVE7AY-"
      },
      "outputs": [],
      "source": [
        "# oversample training samples for crossvalidation\n",
        "import pandas as pd\n",
        "\n",
        "# load relevant packages\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "\n",
        "# import SMOTE package\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "tweets = pd.read_csv(\"https://www.dropbox.com/s/gv56nu1ptrp63ps/annotated_german_refugee_tweets.csv?dl=1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide into training and test set."
      ],
      "metadata": {
        "id": "kTuo8ET95khB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# divide into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    ___,\n",
        "    ___,\n",
        "    test_size=___,\n",
        "    random_state=42\n",
        ")\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)"
      ],
      "metadata": {
        "id": "dgPuLa2JfnMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How imbalanced is the data?"
      ],
      "metadata": {
        "id": "AJWSJtPP5vCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{sum(y_train)} positive cases out of a full set of {len(y_train)} in the training set ({round(sum(y_train)*100/len(y_train), 2)}%).\")\n",
        "pd.Series(y_train).value_counts().plot(kind='bar', title='Class distribution in raw training set')"
      ],
      "metadata": {
        "id": "203HS_NL5sgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only 3% of the training data belong to the class of interest! Can we fix this by applying SMOTE?"
      ],
      "metadata": {
        "id": "wPK5FWOq529v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How does SMOTE work?"
      ],
      "metadata": {
        "id": "elaOgH4e4iJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doesn't look great, huh? We could just undersample, meaning we drop cases from the majority class (in this cases tweets *not* about refugees) until we have a balanced data set. However, we would end up with only a fraction of the observations and would lose a lot of information.\n",
        "\n",
        "SMOTE instead **over**samples the minority class \"by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of the k minority class nearest neighbors\" (p.328). \n",
        "\n",
        "In English: We take one observation in the minority class (in this case tweets) and look for similar minority class observations using a *k-nearest-neighbors* approach (more on this approach [here](https://medium.com/towards-data-science/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761)). For a random subset of these similar neighbors, we generate a synthetic observation somewhere in the middle between the original observation and the chosen neighbor (see visualisation below, shamelessly stolen from the scikit-learn documentation).\n",
        "\n",
        "![](https://imbalanced-learn.org/stable/_images/sphx_glr_plot_illustration_generation_sample_001.png)"
      ],
      "metadata": {
        "id": "ES9Su1RRgQQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application"
      ],
      "metadata": {
        "id": "uMeUqBub4uHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enough theory, let's just see how this works! First, we need to transform our texts into numbers by translating them into a document-term-matrix. For simplicity, we use the simple count-based matrix here, but you might use tfidf or any other method that produces some number indicating the frequency of a word in a document as well."
      ],
      "metadata": {
        "id": "W1xw3p1UlW9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define Vectorizer\n",
        "vec = TfidfVectorizer() # usually we would add some arguments here\n",
        "X_train_dtm = vec.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "E79uE8LpmsW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now witness the beauty of simplicity:"
      ],
      "metadata": {
        "id": "w8Op-oZUnJ_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resample with SMOTE ()\n",
        "X_resample, y_resample = SMOTE(sampling_strategy = _).fit_resample(__, __)"
      ],
      "metadata": {
        "id": "dDswDEg8gPnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{sum(y_resample)} positive cases out of a full set of {len(y_resample)} in the training set ({sum(y_resample)/len(y_resample)*100}%).\")\n",
        "pd.Series(y_resample).value_counts().plot(kind='bar', title='Class distribution in raw training set')"
      ],
      "metadata": {
        "id": "MeNkk3zi9wAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://media3.giphy.com/media/zIwIWQx12YNEI/giphy.gif?cid=ecf05e47yuqbrs13qjl921umf83eb1m4nhnpcstrc38zvcuk&rid=giphy.gif&ct=g)"
      ],
      "metadata": {
        "id": "FJrE3Skpnp8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance"
      ],
      "metadata": {
        "id": "bUlUDh305Cal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we turned our 3% minority class into 50%! (Note: more careful design of this ratio should lead to better results)\n",
        "\n",
        "Let's see how it performs. First, we train a classifier on the unSMOTE'd, imbalanced data:"
      ],
      "metadata": {
        "id": "WLInW20Elhuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import \n",
        "\n",
        "# fit\n",
        "pipe = Pipeline([('tfidf', TfidfVectorizer()), ('', )])\n",
        "pipe.___(X_train, y_train)\n",
        "\n",
        "# classify test set and show performance\n",
        "y_pred_m = pipe.___(X_test)\n",
        "\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred_m).round(2))\n",
        "print(\"Recall: \", recall_score(y_test, y_pred_m).round(2))\n",
        "print(\"Precision: \", precision_score(y_test, y_pred_m).round(2))\n",
        "print(\"F1: \", f1_score(y_test, y_pred_m).round(2))\n",
        "pd.crosstab(y_test, y_pred_m)"
      ],
      "metadata": {
        "id": "nExRFEJNrqYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have very good precision, but would only find very few of the cases in the outcome category, which are the ones we care about after all. "
      ],
      "metadata": {
        "id": "mDeiWfb3y_Th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How 'bout SMOTE?"
      ],
      "metadata": {
        "id": "OrVBooPl7bal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform test set\n",
        "X_test_dtm = vec.___(X_test) \n",
        "## note it's transform, not fit_transform, as we transform it into the same dtm as the training set\n",
        "\n",
        "# define and fit classifier\n",
        "clsfr = ___\n",
        "clsfr.fit(____, _____)\n",
        "\n",
        "# predict\n",
        "y_pred = clsfr.predict(X_test_dtm)\n",
        "\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"Recall: \", round(recall_score(y_test, y_pred), 2))\n",
        "print(\"Precision: \", round(precision_score(y_test, y_pred), 2))\n",
        "print(\"F1: \", round(f1_score(y_test, y_pred), 2))\n",
        "pd.crosstab(y_test, y_pred)"
      ],
      "metadata": {
        "id": "kMnnyL_moUnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE for the win!\n",
        "\n",
        "---\n",
        "\n",
        "For more info on SMOTE and its variants, check out the [scikit-learn documentation on combining under- and oversampling](https://imbalanced-learn.org/stable/combine.html)."
      ],
      "metadata": {
        "id": "OCCC-6CXr-Qj"
      }
    }
  ]
}